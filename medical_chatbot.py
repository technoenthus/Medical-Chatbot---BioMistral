# -*- coding: utf-8 -*-
"""Medical Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YT0BWEe4GmMjuBPAC5LyDsGpa0Wpcv96
"""

from google.colab import drive
drive.mount("/content/drive")

!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf

from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain.vectorstores import Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/BioMistral/Data")
docs = loader.load()

len(docs) #pg no.

docs[5]

#chunking
splitter=RecursiveCharacterTextSplitter(chunk_size=300,chunk_overlap=50)
chunks=splitter.split_documents(docs)

len(chunks)   #so we have 747 chunks of 300 characters each in this doc

import os

#Embedding creations
os.environ['HUGGINGFACEHUB_API_TOKEN']="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"

embeddings=SentenceTransformerEmbeddings(model_name="NeuML/pubmedbert-base-embeddings")

vectorstore=Chroma.from_documents (chunks, embeddings)

query="Who is at risk of heart disease?"
search_results = vectorstore.similarity_search(query)
search_results

retriever=vectorstore.as_retriever(search_kwargs={'k':5})
retriever.get_relevant_documents (query)



# from langchain_community.llms import LlamaCpp

llm = LlamaCpp(
    model_path="/content/drive/MyDrive/BioMistral/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.2,
    max_tokens=2048,
    top_p=1
)

from google.colab import drive
drive.mount('/content/drive')

#use llm and retriver
template="""
<|context|>
You are a Medical Assistant that follows the instructions and generate the accurate response based on the query and context provided
Please be truthful and give direct answers to query.
</s>
<|user|>
{query}
</s>
<|assistant>
"""

from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

prompt=ChatPromptTemplate.from_template(template)

rag_chain = (
{"context": retriever, "query": RunnablePassthrough()}
| prompt
| llm
| StrOutputParser()
)

response=rag_chain.invoke(query)

response

import sys
while True:
  user_input = input(f"Input query: ")
  if user_input == 'exit':
    print("Exiting...")
    sys.exit()
  if user_input=="":
    continue
  result = rag_chain.invoke(user_input)
  print("Answer: ", result)